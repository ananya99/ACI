defaults:
  # Set to use submitit_slurm
  - /hydra/launcher/submitit_slurm@_here_

# Set the timeout of each individual job to 1 day. Note that this isn't the total job
# length. It is recommended to start the "daemon" job using srun/sbatch on a low
# compute node. This way, you can limit the total job length to some maximum value.
timeout_min: ${eval:'60*24'} # 1 day (in min)

# Supercloud only allows allocation of 40 cpus and 8 gpus at any one time. To optimize
# usage of resources, we want to use all 40 cpus and 8 gpus and also have multiple
# parallel workers on each node. This can manifest in a few ways:
#   - we could have multiple parallel environments running on the same node which
#     provides more training samples per update at the cost of sample efficiency
#   - or we could have multiple trainers running on the same node which provides more
#     updates per time at the cost of compute efficiency
# We'll dynamically select the number of tasks_per_node (i.e. the number of trainers
# running on each node) based on the number of parallel environments each trainer.
# Each supercloud node only has 40 cpus, so we'll set the number of tasks_per_node to
# 20 // cpus_per_task. This will allow us to run the maximum number of trainers on each
# node while still allowing for another trainer to run on the same node. In this way,
# there will be 8 total trainers running on each node (1 per gpu, each with 20 cpus).
nodes: 1
tasks_per_node: ${eval:'int(20 // ${.cpus_per_task})'}
cpus_per_task: ${trainer.n_envs}
gres: gpu:volta:1
partition: xeon-g6-volta
array_parallelism: 8 # number of parallel array jobs launched; sc only allows 8 gpus

# This will increase the likelyhood of a job being selected during peak hours.
# qos: high

# these are run prior to calling sbatch before running the job
setup:
  - source /etc/profile
  - export TF_CPP_MIN_LOG_LEVEL=2
  - export OPENBLAS_NUM_THREADS=1
  - export PMIX_MCA_gds=hash
