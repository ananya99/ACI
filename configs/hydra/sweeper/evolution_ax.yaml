defaults:
  # We use ax for the evolution strategy
  - /hydra/sweeper/ax@_here_

# NOTE: hydra sweepers are _not_ asynchronous in that it will wait until all workers are
# finished before selecting the next parameters to optimize over. For example, when
# using the submitit_slurm launcher with num_workers=4, all 4 jobs will be run and
# synchronously waited for all to finish until moving on. This may not be desired.
# Possible work arounds:
# 1. use a different launcher which requires one worker from the sweeper
#    this would involve writing a custom launcher tho I think
# 2. run more workers than the number of jobs you want to run in parallel. this means
#    they will effectively wait in the queue until the next job is ready to run
#    which is kind of asynchronous. Problem would be that each job that is queued
#    wasn't "intellegently" selected by the sweeper.
# 3. make a PR for hydra adding support for asynchronous launches. See this comment:
#    https://github.com/facebookresearch/hydra/blame/main/plugins/\
#       hydra_nevergrad_sweeper/hydra_plugins/hydra_nevergrad_sweeper/_impl.py#L162

# This defines the number of a parallel candidates you want to test at one time.
# This will default to 1 for most launchers. If the launcher is submitit, it will
# evaluate to batch_size * array_parallelism, which is basically the number of
# parallel jobs to run on each node * number of total nodes
max_batch_size: |
  ${eval:'${oc.select:hydra.launcher.tasks_per_node, 1} * ${oc.select:hydra.launcher.array_parallelism, 1}'}

ax_config:
  # The budget is the number of evaluations we want to run. The total number of
  # generations is the budget divided by the number of workers. We'll set the
  # number generations and calculate the budget from that. Default generations to 20.
  # NOTE: If budget is not a direct multiple of num_workers, at the end, the sweeper
  # will run the remaining number of workers to reach the budget, which messes with
  # our current generation calculation.
  # NOTE #2: Some algorithms use the budget and num_workers as part of the optimization
  # to know how exploratory to be. So be careful setting this number very large with
  # the intention of stopping early.
  max_trials: ${eval:'int(${oc.select:evo.num_generations,20} * ${.num_workers})'}

  early_stop:
    # Set to the experiment minimize. To override, update experiment.minimize
    minimize: ${..experiment.minimize}

    # Set the maximum number of epochs without improvement to 5. We have a very
    # computationally instensive objective function, and we don't want to evaluate
    # it more times than we need to.
    max_epochs_without_improvement: 5

    # Set the epsilon which indicates the minimum improvement in the objective function
    # to be considered an improvement. We'll set it to 1e-6.
    epsilon: 1e-6

  experiment:
    name: ${expname}

    # Since we may want to define constraints, we'll return the fitness as a dictionary
    # where this is the key of the fitness value.
    objective_name: "fitness"

    # Here we assume we want to maximize the fitness returned by trainer. This may not
    # always be the case, but can be overridden in the downstream configs.
    minimize: false

  client:
    # This seed is only used at the very beginning and is used for reproducibility of
    # the underlying algorithm, not of the individual runs. We'll set it to 0 here
    # instead of ${seed} because otherwise we get an error saying that the hydra config
    # is unset since the sweeper is evaluated before the hydra config is parsed/set.
    random_seed: 0

  # We expect the same input parameters to produce different output fitness due to
  # seeds and randomness in the training process. Therefore, we set noisy to true.
  is_noisy: true
